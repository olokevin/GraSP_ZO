GraSP:
  exp_name: "scratch_STP_mom0.9"
  network: ttm
  depth: 2
  dataset: mnist
  batch_size: 64
  epoch: 200
  learning_rate: 0.001
  weight_decay: 1e-4
  exception: -1
  iterations: 1
  normalize: false
  target_ratio: 0.90
  pruner: False
  pruner_file: False
  num_classes: 10
  samples_per_class: 10
  samples_batches: 5
  num_iters: 1

optimizer:
  name: ZO_SCD_batch
  grad_estimator: batch
  grad_sparsity: 0.5
  opt_layers_strs: 'TensorizedLinear_module'
  STP: True
  debug: False
  momentum: 0.9
  weight_decay: 0.0001
  # adam: True
  # beta_1: 0.9
  # beta_2: 0.98

scheduler:
  name: 'PresetLRScheduler'
  lr_schedule: {0: 1,
                15: 0.1,
                75: 0.05,
                120: 0.01}

# scheduler:
#   name: 'ExponentialLR'
#   gamma: 0.98

run:
  runs: None
  n_epochs: 100
  batch_size: 32
  use_cuda: 1
  gpu_id: 0
  deterministic: 1
  log_interval: 200
  train_noise: 0

pretrained:
  incre: False
  # 88.91
  # load_model_path: './runs/pruning/mnist/ttm/ZO_SCD_batch/False/scratch_two_bz64_grad0.5_lr_0.01/20230329-080506/run_None/finetune_mnist_ttm2_r0.9_it0_best.pth.tar'
  # 91.31
  load_model_path: './runs/pruning/mnist/ttm/ZO_SCD_batch/False/incre_bz64_grad0.5_lr_0.001/20230329-092109/run_None/finetune_mnist_ttm2_r0.9_it0_best.pth.tar'
  # 93.06
  # load_model_path: './runs/pruning/mnist/ttm/ZO_SCD_batch/False/incre_mom0.9_bz64_grad0.5_lr_0.001/20230329-183037/run_None/finetune_mnist_ttm2_r0.9_it0_best.pth.tar'
  pruned: False

model:
  tensor_type: 'TensorTrainMatrix'
  max_rank: 4
  tensorized: 'TensorizedLinear_module'

checkpoint:
  save_model: "model/MNIST_notensor_SCD"
  save_mode: "best"
  proj_share_weight: 1
  label_smoothing: 1
